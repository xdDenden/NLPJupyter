{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa22cb6e25cb7f7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T15:29:46.846571400Z",
     "start_time": "2026-02-26T15:29:37.273769800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denden\\AppData\\Local\\Programs\\Python\\Python312\\python.exe\n",
      "3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Python 3.11 required. You are running 3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m required = (\u001b[32m3\u001b[39m, \u001b[32m11\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.version_info != required:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     10\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPython \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequired[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m required. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Python 3.11 required. You are running 3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]"
     ]
    }
   ],
   "source": [
    "# [Cell 1]\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from spacy import displacy\n",
    "\n",
    "# Import your custom modules\n",
    "from config import COLORS\n",
    "from ner_model import TransformerNERWithCRF\n",
    "from main import (\n",
    "    predict_with_custom_model,\n",
    "    predict_with_pipeline,\n",
    "    merge_dosages,\n",
    "    # Import your newly refactored evaluation function\n",
    "    run_evaluation\n",
    ")\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "required = (3, 11)\n",
    "if sys.version_info != required:\n",
    "    raise RuntimeError(\n",
    "        f\"Python {required[0]}.{required[1]} required. \"\n",
    "        f\"You are running {sys.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb708dbe215ca3",
   "metadata": {},
   "source": [
    "# Biomedical Named Entity Recognition (BioNER): Baseline Comparison & Fine Tuning\n",
    "\n",
    "In this notebook, we explore the performance of various models on the BC5CDR dataset, a benchmark for biomedical named entity recognition (BioNER). We will compare different BERT's with a particular focus on the `tner/roberta-large-bc5cdr` model as our gold standard. Finally, we will present the results of our own fine tuned model and discuss its performance in relation to the baselines. We wanted to see how close we could get to the SOTA and see which other baseline models could be competitive with the `tner` model, which is a large, fine tuned transformer specifically optimized for this task.\n",
    "\n",
    "## Project Overview & Methodology\n",
    "\n",
    "### 1.The Baselines:\n",
    "Named Entity Recognition in the biomedical domain is notoriously tricky due to nested entities, multi word structures, and heavy use of specific jargon. To understand our model's true performance, we are evaluating it against the following baselines:\n",
    "* **RoBERTa**: https://huggingface.co/FacebookAI/roberta-base\n",
    "* **BioBERT:** https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    "* **SciBERT:** https://huggingface.co/allenai/scibert_scivocab_uncased\n",
    "* **PubMedBERT:** https://huggingface.co/NeuML/pubmedbert-base-embeddings\n",
    "* **BiomedRoBERTA:** https://huggingface.co/allenai/biomed_roberta_base\n",
    "\n",
    "### 2.The Gold Standard: `tner/roberta-large-bc5cdr`\n",
    "This model serves as our primary ceiling. Fine tuned extensively on the BC5CDR dataset by the T-NER library, it represents a highly optimized, SOTA approach for this specific task.\n",
    "Self reported score:\n",
    "* **F1 Score (micro):** ~0.884\n",
    "* **Chemical Entity F1:** ~0.925\n",
    "* **Disease Entity F1:** ~0.833\n",
    "Score we actually measured:\n",
    "* **F1 Score (micro):** ~0.922\n",
    "* **Chemical Entity F1:** ~0.930\n",
    "* **Disease Entity F1:** ~0.913\n",
    "\n",
    "### 3. Our Fine-Tuned Model\n",
    "*So, how close did we get?* We took the \"roberta-base\" and fine-tuned it on the same BC5CDR dataset. Our goal was to see if we could close the gap with the much larger `tner` model (at least 4 times larger). This is more of an exercise in understanding the fine tuning process and the impact of domain specific pre training rather than just chasing the highest score. From the outset we knew the 'tner' model would be hard to beat, but we wanted to give it our best try.\n",
    "\n",
    "### 4. The infrastructure:\n",
    "We used the Hugging Face Transformers library but had to built our own infrastructure that could allow us to easily swap out different models and compare their performance on the same dataset. We also implemented a custom evaluation pipeline to ensure that we were measuring performance consistently across all models. Additionally we implemented a config file that allows us to turn extra layers like a CRF on or off and we implemented an automatic report generator that can take the results of the current run and generate a HTML based report that can be shared with others and easily compared with previous runs. Each report contains a random sample of test sentences as well as specific custom sentences. We will allow you to generate your own report and allow you to test custom sentences later in this notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T15:29:50.779077700Z",
     "start_time": "2026-02-26T15:29:50.728494Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Cell 2]\n",
    "import inspect\n",
    "\n",
    "print(\"Our custom backbone-agnostic Transformer + CRF implementation:\")\n",
    "print(\"-\" * 60)\n",
    "# This will print the source code of your class directly into the notebook output!\n",
    "print(inspect.getsource(TransformerNERWithCRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e0af8206c7f78c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T15:29:54.914897200Z",
     "start_time": "2026-02-26T15:29:54.853377400Z"
    }
   },
   "outputs": [],
   "source": [
    "# [Cell 3]\n",
    "model_options = {\n",
    "    \"RoBERTa Base\": \"roberta-base\",\n",
    "    \"BioBERT Base\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
    "    \"SciBERT\": \"allenai/scibert_scivocab_uncased\",\n",
    "    \"PubMedBERT\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    \"Biomed RoBERTa\": \"allenai/biomed_roberta_base\"\n",
    "}\n",
    "\n",
    "PRETRAINED_BASELINE = \"tner/roberta-large-bc5cdr\"\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=\"allenai/biomed_roberta_base\",\n",
    "    description='Select Model:',\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Generate Report\", button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_model = dropdown.value\n",
    "        print(f\"Loading pipeline for {selected_model}... This may take a moment.\")\n",
    "\n",
    "        run_evaluation(selected_model, PRETRAINED_BASELINE)\n",
    "\n",
    "        print(f\"Report successfully generated for {selected_model}!\")\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(dropdown, button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc1fe8b5bb171da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cell 4]\n",
    "# Assuming model_ours, tokenizer_ours, and pipe_pre are loaded in memory\n",
    "# (You can load them in a hidden cell prior to this one)\n",
    "\n",
    "text_area = widgets.Textarea(\n",
    "    value='The patient was prescribed 50mg of Aspirin for the severe headache.',\n",
    "    placeholder='Enter clinical text here...',\n",
    "    description='Text:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "eval_button = widgets.Button(description=\"Extract Entities\", button_style='info')\n",
    "eval_output = widgets.Output()\n",
    "\n",
    "def on_eval_clicked(b):\n",
    "    with eval_output:\n",
    "        eval_output.clear_output()\n",
    "        text = text_area.value\n",
    "        opts = {\"colors\": COLORS}\n",
    "\n",
    "        # 1. Custom CRF Model\n",
    "        our_preds = predict_with_custom_model(text, model_ours, tokenizer_ours)\n",
    "        print(\"Our Custom CRF Model:\")\n",
    "        displacy.render({\"text\": text, \"ents\": merge_dosages(our_preds, text)},\n",
    "                        style=\"ent\", manual=True, options=opts, jupyter=True)\n",
    "\n",
    "        # 2. Baseline Model\n",
    "        pre_preds = predict_with_pipeline(text, pipe_pre)\n",
    "        print(\"\\nPretrained Baseline:\")\n",
    "        displacy.render({\"text\": text, \"ents\": merge_dosages(pre_preds, text)},\n",
    "                        style=\"ent\", manual=True, options=opts, jupyter=True)\n",
    "\n",
    "eval_button.on_click(on_eval_clicked)\n",
    "display(text_area, eval_button, eval_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
