{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ebb708dbe215ca3",
   "metadata": {},
   "source": [
    "# Biomedical Named Entity Recognition (BioNER): Baseline Comparison & Fine Tuning\n",
    "\n",
    "In this notebook, we explore the performance of various models on the BC5CDR dataset, a benchmark for biomedical named entity recognition (BioNER). We will compare different BERT's with a particular focus on the `tner/roberta-large-bc5cdr` model as our gold standard. Finally, we will present the results of our own fine tuned model and discuss its performance in relation to the baselines. We wanted to see how close we could get to the SOTA and see which other baseline models could be competitive with the `tner` model, which is a large, fine tuned transformer specifically optimized for this task.\n",
    "\n",
    "## Project Overview & Methodology\n",
    "\n",
    "### 1.The Baselines:\n",
    "Named Entity Recognition in the biomedical domain is notoriously tricky due to nested entities, multi word structures, and heavy use of specific jargon. To understand our model's true performance, we are evaluating it against the following baselines:\n",
    "* **RoBERTa**: https://huggingface.co/FacebookAI/roberta-base\n",
    "* **BioBERT:** https://huggingface.co/dmis-lab/biobert-base-cased-v1.2\n",
    "* **SciBERT:** https://huggingface.co/allenai/scibert_scivocab_uncased\n",
    "* **PubMedBERT:** https://huggingface.co/NeuML/pubmedbert-base-embeddings\n",
    "* **BiomedRoBERTA:** https://huggingface.co/allenai/biomed_roberta_base\n",
    "\n",
    "### 2.The Gold Standard: `tner/roberta-large-bc5cdr`\n",
    "This model serves as our primary ceiling. Fine tuned extensively on the BC5CDR dataset by the T-NER library, it represents a highly optimized, SOTA approach for this specific task.\n",
    "Self reported score:\n",
    "* **F1 Score (micro):** ~0.884\n",
    "* **Chemical Entity F1:** ~0.925\n",
    "* **Disease Entity F1:** ~0.833\n",
    "Score we actually measured:\n",
    "* **F1 Score (micro):** ~0.922\n",
    "* **Chemical Entity F1:** ~0.930\n",
    "* **Disease Entity F1:** ~0.913\n",
    "\n",
    "### 3. Our Fine-Tuned Model\n",
    "*So, how close did we get?* We took the \"roberta-base\" and fine-tuned it on the same BC5CDR dataset. Our goal was to see if we could close the gap with the much larger `tner` model (at least 4 times larger). This is more of an exercise in understanding the fine tuning process and the impact of domain specific pre training rather than just chasing the highest score. From the outset we knew the 'tner' model would be hard to beat, but we wanted to give it our best try.\n",
    "\n",
    "### 4. The infrastructure:\n",
    "We used the Hugging Face Transformers library but had to built our own infrastructure that could allow us to easily swap out different models and compare their performance on the same dataset. We also implemented a custom evaluation pipeline to ensure that we were measuring performance consistently across all models. Additionally we implemented a config file that allows us to turn extra layers like a CRF on or off and we implemented an automatic report generator that can take the results of the current run and generate a HTML based report that can be shared with others and easily compared with previous runs. Each report contains a random sample of test sentences as well as specific custom sentences. We will allow you to generate your own report and allow you to test custom sentences later in this notebook!\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Firstly lets make sure we have the right python version and all the necessary libraries installed. We will be using Python 3.11 for this project. Please execute the following cell to check your python version and ensre you have all the required libraries installed. If you are missing any libraries, you can install them using pip.",
   "id": "c1911f61d0bb80c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T21:04:54.560458300Z",
     "start_time": "2026-02-26T21:04:27.245350100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# [Cell 1]\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from spacy import displacy\n",
    "\n",
    "# Import your custom modules\n",
    "from config import COLORS\n",
    "from ner_model import TransformerNERWithCRF\n",
    "from main import (\n",
    "    predict_with_custom_model,\n",
    "    predict_with_pipeline,\n",
    "    merge_dosages,\n",
    "    # Import your newly refactored evaluation function\n",
    "    run_evaluation\n",
    ")\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "\n",
    "required = (3, 11)\n",
    "\n",
    "if sys.version_info[:2] != required:\n",
    "    raise RuntimeError(\n",
    "        f\"Python {required[0]}.{required[1]}.x required. \"\n",
    "        f\"You are running {sys.version}\"\n",
    "    )"
   ],
   "id": "c805f334e7da377f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Denden\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n",
      "3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A quick word on the bc5cdr dataset (BioCreative-V-CDR-Corpus). This dataset is a widely used benchmark for biomedical named entity recognition (BioNER). It contains a large collection of biomedical abstracts from PubMed annotated with chemical and disease entities. https://huggingface.co/datasets/bigbio/bc5cdr.",
   "id": "e78ba766766916b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We believe the best way to start is by getting an initial understanding of the performance of the different models on the BC5CDR dataset. We have implemented a function called `run_evaluation` that will train and evaluate all the models we are interested in and generate a report with the results. This function is located in our `main.py` file and it takes care of all the training, evaluation, and report generation. You can select any of the models we have listed in the dropdown and it will be trained and evaluated against the `tner/roberta-large-bc5cdr` model. Please note that this process can take some time, especially for the larger models, so please be patient while it runs especially if this is a first run. The model will download, train on the dataset and then benchmark itself and the pretrained large roBERTa model on the test set. Once the evaluation is complete, it will generate a report that you can view in your browser. The report will contain a random sample of test sentences as well as specific custom sentences that we have selected to highlight the differences between the models. You can also generate your own custom sentences later in this notebook.\n",
   "id": "e772cf1abef19c7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T21:08:54.203132900Z",
     "start_time": "2026-02-26T21:08:54.078948300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# [Cell 3]\n",
    "model_options = {\n",
    "    \"RoBERTa Base\": \"roberta-base\",\n",
    "    \"BioBERT Base\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
    "    \"SciBERT\": \"allenai/scibert_scivocab_uncased\",\n",
    "    \"PubMedBERT\": \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    \"Biomed RoBERTa\": \"allenai/biomed_roberta_base\"\n",
    "}\n",
    "\n",
    "PRETRAINED_BASELINE = \"tner/roberta-large-bc5cdr\"\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=\"allenai/biomed_roberta_base\",\n",
    "    description='Select Model:',\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Generate Report\", button_style='success')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_model = dropdown.value\n",
    "        print(f\"Training and evaluating based on config.py... This will take time.\")\n",
    "\n",
    "        # PASS THE RELATIVE PATH DIRECTLY HERE\n",
    "        run_evaluation(\n",
    "            model_path=\"./final_clinical_ner_crf_model\",\n",
    "            baseline_path=PRETRAINED_BASELINE,\n",
    "            train_first=True\n",
    "        )\n",
    "\n",
    "        print(\"Report successfully generated!\")\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(dropdown, button, output)"
   ],
   "id": "221ffc013c87ef0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropdown(description='Select Model:', index=4, options={'RoBERTa Base': 'roberta-base', 'BioBERT Base': 'dmis-…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23cb5e0e62a840a69a7a176d40d43f0f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Button(button_style='success', description='Generate Report', style=ButtonStyle())"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0a79d1e4a3c4454abe3383d9faf5a91"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0bc436c94214f4081c5c00510b9e652"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Please note the finished report and statistics will be saved in the reports folder. Its the same folder where this jupyter notebook is located. You can open the report in your browser and see the results.",
   "id": "72d1697f0dc37267"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Regardless of which model you chose to compare against, it should be obvious that the `tner/roberta-large-bc5cdr` model is very good at recognizing chemical and disease entities in the text and understanding that many diseases might be made up of multiple words. (Also please note we extract the dosage from each sentence but this is done via RegEx and does not affect the performance of the models) Given the model you chose there should be a 40-50% gap between the performance of the model you chose and the 'tner' model. The classification of diseases and chemicals provides the extra challenge of compound words. For example as you can see in the sentence \"The patient was prescribed 50mg of Aspirin for the severe headache.\" the model needs to recognize that \"severe headache\" is a disease entity and not just \"headache\". This is where the `tner` model really shines and is able to capture these multi word entities much better than the other models. The `tner` model is also able to capture more complex relationships between entities, such as recognizing that \"Aspirin\" is a chemical entity and that it is being used to treat the \"severe headache\". This is where the fine tuning on the BC5CDR dataset really pays off and allows the model to learn the specific language and structure of biomedical text.\n",
    "\n",
    "Let us now consider why compound words pose a challenge to NER models. In general, NER models are trained to recognize individual words as entities. Usually older models use rigid, space based tokenization and static word embeddings. This means that any compound word gets automatically treated as one word. If the model has never seen that specific compound word during training it will be classified as unknown and create \"out of vocab\" (OOV) issues. This even happens if the model has seen each individual word of the compound word. Transformer models like BERT or RoBERa use subword tokenization algorithms like WordPiece or BPE. Instead of failing on an unseen word with a OOV error it will break down the compound words into smaller subword units that it does recognize. The self attention mechanism also allows for the model to capture the relationships between these subword units and understand context that older models simply could not do."
   ],
   "id": "52056d93959109a7"
  },
  {
   "cell_type": "code",
   "id": "6bc1fe8b5bb171da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-26T21:16:06.589466400Z",
     "start_time": "2026-02-26T21:16:06.526074700Z"
    }
   },
   "source": [
    "# [Cell 4]\n",
    "# Assuming model_ours, tokenizer_ours, and pipe_pre are loaded in memory\n",
    "# (You can load them in a hidden cell prior to this one)\n",
    "\n",
    "text_area = widgets.Textarea(\n",
    "    value='The patient was prescribed 50mg of Aspirin for the severe headache.',\n",
    "    placeholder='Enter clinical text here...',\n",
    "    description='Text:',\n",
    "    layout=widgets.Layout(width='80%', height='80px')\n",
    ")\n",
    "\n",
    "eval_button = widgets.Button(description=\"Extract Entities\", button_style='info')\n",
    "eval_output = widgets.Output()\n",
    "\n",
    "def on_eval_clicked(b):\n",
    "    with eval_output:\n",
    "        eval_output.clear_output()\n",
    "        text = text_area.value\n",
    "        opts = {\"colors\": COLORS}\n",
    "\n",
    "        # 1. Custom CRF Model\n",
    "        our_preds = predict_with_custom_model(text, model_ours, tokenizer_ours)\n",
    "        print(\"Our Custom CRF Model:\")\n",
    "        displacy.render({\"text\": text, \"ents\": merge_dosages(our_preds, text)},\n",
    "                        style=\"ent\", manual=True, options=opts, jupyter=True)\n",
    "\n",
    "        # 2. Baseline Model\n",
    "        pre_preds = predict_with_pipeline(text, pipe_pre)\n",
    "        print(\"\\nPretrained Baseline:\")\n",
    "        displacy.render({\"text\": text, \"ents\": merge_dosages(pre_preds, text)},\n",
    "                        style=\"ent\", manual=True, options=opts, jupyter=True)\n",
    "\n",
    "eval_button.on_click(on_eval_clicked)\n",
    "display(text_area, eval_button, eval_output)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Textarea(value='The patient was prescribed 50mg of Aspirin for the severe headache.', description='Text:', lay…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bc477245ef940a2bd3f007d249dc0ea"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Button(button_style='info', description='Extract Entities', style=ButtonStyle())"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cb115d207854a858af25f85720c3fbd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d70d51a42174f4f8c168935b7676169"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###",
   "id": "b635aa7bedf6c718"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
